# HomebrewNLP

## Overview

A case study of efficient training of large language models using commodity hardware.

## Example Command

```BASH
python3 main.py train --config_path configs/small.yaml
```

---
[![DeepSource](https://deepsource.io/gh/HomebrewNLP/HomebrewNLP.svg/?label=active+issues&show_trend=true&token=sAQ42SRyNPilkjj82sQd88ea)](https://deepsource.io/gh/HomebrewNLP/HomebrewNLP/?ref=repository-badge)
| [Discord](https://discord.gg/JSGG6Abcyx)
| [WandB](https://wandb.ai/homebrewnlp/gpt)

## Datasets
* [Book Dataset](https://drive.google.com/file/u/1/d/1aoW3KI2E3nK7B28RE6I6_oDtNidTvoc2/view?usp=sharing)
* [200MB Slice](https://drive.google.com/file/d/1QTbRYe-BOq2kw8foWB16NGPthQjZr7yn/view?usp=sharing) of [ThePile](https://github.com/EleutherAI/the-pile)




## Citing

### BibTeX

```bibtex
@misc{nestler2021homebrewnlp,
  title = {{HomebrewNLP}},
  author = {Nestler, Lucas and Gill, David},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.5553247},
  howpublished = {\url{https://github.com/HomebrewNLP/HomebrewNLP}}
}
```

### Latest DOI

[![DOI](https://zenodo.org/badge/279888521.svg)](https://zenodo.org/badge/latestdoi/279888521)


